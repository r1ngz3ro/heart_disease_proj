{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Heart Disease Prediction - Data Science Lab Project\n",
    "# \n",
    "# ## Problem Definition & Objectives\n",
    "# \n",
    "# The objective of this project is to predict the presence or absence of heart disease using the UCI Heart Disease Dataset. This is a binary classification problem where we aim to determine whether a patient has heart disease based on various clinical attributes.\n",
    "# \n",
    "# **Target Variable:** \n",
    "# - `num`: diagnosis of heart disease (angiographic disease status)\n",
    "#   - Value 0: < 50% diameter narrowing (no heart disease)\n",
    "#   - Value 1: > 50% diameter narrowing (heart disease present)\n",
    "# \n",
    "# **Motivation:**\n",
    "# - Early detection of heart disease can significantly improve patient outcomes\n",
    "# - Machine learning models can assist healthcare professionals in making diagnostic decisions\n",
    "# - Understanding the key factors that contribute to heart disease risk\n",
    "# \n",
    "# **Key Features:**\n",
    "# - Age: patient's age in years\n",
    "# - Sex: gender (1 = male; 0 = female)\n",
    "# - Chest pain type: type of chest pain experienced\n",
    "# - Resting blood pressure: resting blood pressure (in mm Hg)\n",
    "# - Serum cholesterol: serum cholestoral in mg/dl\n",
    "# - Fasting blood sugar: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "# - Resting ECG results: resting electrocardiographic results\n",
    "# - Maximum heart rate achieved: maximum heart rate achieved during exercise\n",
    "# - Exercise induced angina: (1 = yes; 0 = no)\n",
    "# - ST depression: ST depression induced by exercise relative to rest\n",
    "# - Slope: the slope of the peak exercise ST segment\n",
    "# - Number of vessels: number of major vessels colored by fluoroscopy\n",
    "# - Thal: results of thallium stress test (3 = normal; 6 = fixed defect; 7 = reversable defect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "# The processed Cleveland dataset contains 14 key attributes as specified in the documentation\n",
    "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', \n",
    "                'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "\n",
    "df = pd.read_csv('processed.cleveland.data', names=column_names, na_values='?')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb74605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nDataset Description:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)\n",
    "\n",
    "\n",
    "# ## Exploratory Data Analysis (EDA)\n",
    "# \n",
    "# Let's explore the dataset to understand the distribution of variables, relationships between features, and characteristics of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc259d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "target_counts = df['num'].value_counts()\n",
    "plt.bar(target_counts.index, target_counts.values)\n",
    "plt.title('Distribution of Heart Disease Diagnosis')\n",
    "plt.xlabel('Diagnosis (0=No Disease, 1-4=Disease)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Convert to binary classification (0 vs 1-4)\n",
    "df_binary = df.copy()\n",
    "df_binary['num_binary'] = df_binary['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "target_binary_counts = df_binary['num_binary'].value_counts()\n",
    "plt.bar(target_binary_counts.index, target_binary_counts.values)\n",
    "plt.title('Binary Classification Distribution\\n(0=No Disease, 1=Disease)')\n",
    "plt.xlabel('Diagnosis (0=No Disease, 1=Disease)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Binary target distribution:\")\n",
    "print(df_binary['num_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for key variables\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Distribution of Key Variables', fontsize=16)\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df['age'], bins=20, edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "\n",
    "# Sex distribution\n",
    "sex_counts = df['sex'].value_counts()\n",
    "axes[0, 1].bar(sex_counts.index, sex_counts.values)\n",
    "axes[0, 1].set_title('Sex Distribution (0=Female, 1=Male)')\n",
    "axes[0, 1].set_xlabel('Sex')\n",
    "\n",
    "# Chest pain type distribution\n",
    "cp_counts = df['cp'].value_counts()\n",
    "axes[0, 2].bar(cp_counts.index, cp_counts.values)\n",
    "axes[0, 2].set_title('Chest Pain Type Distribution')\n",
    "axes[0, 2].set_xlabel('Chest Pain Type')\n",
    "\n",
    "# Resting blood pressure\n",
    "axes[1, 0].hist(df['trestbps'], bins=20, edgecolor='black')\n",
    "axes[1, 0].set_title('Resting Blood Pressure Distribution')\n",
    "axes[1, 0].set_xlabel('Resting Blood Pressure (mm Hg)')\n",
    "\n",
    "# Cholesterol\n",
    "axes[1, 1].hist(df['chol'], bins=20, edgecolor='black')\n",
    "axes[1, 1].set_title('Cholesterol Distribution')\n",
    "axes[1, 1].set_xlabel('Cholesterol (mg/dl)')\n",
    "\n",
    "# Maximum heart rate\n",
    "axes[1, 2].hist(df['thalach'], bins=20, edgecolor='black')\n",
    "axes[1, 2].set_title('Maximum Heart Rate Distribution')\n",
    "axes[1, 2].set_xlabel('Maximum Heart Rate')\n",
    "\n",
    "# Exercise induced angina\n",
    "exang_counts = df['exang'].value_counts()\n",
    "axes[2, 0].bar(exang_counts.index, exang_counts.values)\n",
    "axes[2, 0].set_title('Exercise Induced Angina Distribution')\n",
    "axes[2, 0].set_xlabel('Exercise Induced Angina')\n",
    "\n",
    "# ST depression\n",
    "axes[2, 1].hist(df['oldpeak'], bins=20, edgecolor='black')\n",
    "axes[2, 1].set_title('ST Depression Distribution')\n",
    "axes[2, 1].set_xlabel('ST Depression')\n",
    "\n",
    "# Number of vessels\n",
    "ca_counts = df['ca'].value_counts()\n",
    "axes[2, 2].bar(ca_counts.index, ca_counts.values)\n",
    "axes[2, 2].set_title('Number of Major Vessels Distribution')\n",
    "axes[2, 2].set_xlabel('Number of Major Vessels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Select only numeric columns for correlation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "print(\"Missing values in each column:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4441dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance analysis for binary classification\n",
    "print(\"Class balance for binary classification (0=No Disease, 1=Disease):\")\n",
    "binary_counts = df_binary['num_binary'].value_counts()\n",
    "print(binary_counts)\n",
    "print(f\"\\nPercentage of patients with heart disease: {binary_counts[1]/(binary_counts[0]+binary_counts[1])*100:.2f}%\")\n",
    "print(f\"Percentage of patients without heart disease: {binary_counts[0]/(binary_counts[0]+binary_counts[1])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between key features and target variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Feature Distributions by Heart Disease Status', fontsize=16)\n",
    "\n",
    "# Age by target\n",
    "df_with_target = df.copy()\n",
    "df_with_target['num_binary'] = df_binary['num_binary']\n",
    "df_with_target.boxplot(column='age', by='num_binary', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age by Heart Disease Status')\n",
    "axes[0, 0].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "\n",
    "# Sex by target\n",
    "pd.crosstab(df['sex'], df_binary['num_binary']).plot(kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Sex by Heart Disease Status')\n",
    "axes[0, 1].set_xlabel('Sex (0=Female, 1=Male)')\n",
    "axes[0, 1].legend(title='Heart Disease')\n",
    "\n",
    "# Chest pain type by target\n",
    "pd.crosstab(df['cp'], df_binary['num_binary']).plot(kind='bar', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Chest Pain Type by Heart Disease Status')\n",
    "axes[0, 2].set_xlabel('Chest Pain Type')\n",
    "axes[0, 2].legend(title='Heart Disease')\n",
    "\n",
    "# Maximum heart rate by target\n",
    "df_with_target.boxplot(column='thalach', by='num_binary', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Maximum Heart Rate by Heart Disease Status')\n",
    "axes[1, 0].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "\n",
    "# Cholesterol by target\n",
    "df_with_target.boxplot(column='chol', by='num_binary', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Cholesterol by Heart Disease Status')\n",
    "axes[1, 1].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "\n",
    "# Exercise induced angina by target\n",
    "pd.crosstab(df['exang'], df_binary['num_binary']).plot(kind='bar', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Exercise Induced Angina by Heart Disease Status')\n",
    "axes[1, 2].set_xlabel('Exercise Induced Angina')\n",
    "axes[1, 2].legend(title='Heart Disease')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## Data Cleaning & Preprocessing\n",
    "# \n",
    "# In this section, we'll handle missing values, encode categorical variables, scale numerical features, and split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33598927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Before preprocessing:\")\n",
    "print(\"Missing values:\", df.isnull().sum().sum())\n",
    "\n",
    "# Replace missing values with median for numerical columns\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            # Use median to fill missing values\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            # Use mode for categorical columns\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(\"Missing values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d860d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target variable (0 = no disease, 1 = disease)\n",
    "df_processed = df.copy()\n",
    "df_processed['target'] = df_processed['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(['num', 'target'], axis=1)\n",
    "y = df_processed['target']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"Feature columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab513c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables if needed\n",
    "# In this dataset, most variables are already numeric but represent categories\n",
    "# We'll treat them as they are since they're already encoded\n",
    "\n",
    "# Check unique values in each column to understand categorical vs numerical\n",
    "print(\"Unique values in each column:\")\n",
    "for col in X.columns:\n",
    "    print(f\"{col}: {sorted(X[col].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Test set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2776bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames to maintain column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Scaled training data shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled test data shape:\", X_test_scaled.shape)\n",
    "\n",
    "\n",
    "# ## Modeling\n",
    "# \n",
    "# We'll train at least one model (Logistic Regression) and optionally a Random Forest model as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model (optional)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest trained successfully!\")\n",
    "\n",
    "\n",
    "# ## Model Evaluation\n",
    "# \n",
    "# We'll evaluate the models using appropriate metrics: accuracy, confusion matrix, and ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "print(\"=== Logistic Regression Results ===\")\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_lr)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "print(\"=== Random Forest Results ===\")\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_rf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db34bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})')\n",
    "\n",
    "# Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.4f})')\n",
    "\n",
    "# Diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909deaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for Logistic Regression\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': log_reg.coef_[0]\n",
    "}).sort_values(by='coefficient', key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=coefficients.head(10), x='coefficient', y='feature')\n",
    "plt.title('Top 10 Feature Coefficients (Logistic Regression)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features (by absolute coefficient value):\")\n",
    "print(coefficients.head(10))\n",
    "\n",
    "\n",
    "# ## Conclusion\n",
    "# \n",
    "# In this section, we'll summarize the key findings, limitations of the study, and possible improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONCLUSION ===\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. The dataset contains\", len(df), \"patient records with 13 features used to predict heart disease.\")\n",
    "print(\"2. The target variable was converted to binary classification (0 = no disease, 1 = disease).\")\n",
    "print(\"3. Missing values were handled by replacing them with median values for numerical features.\")\n",
    "print(\"4. Logistic Regression achieved an accuracy of {:.4f} and ROC-AUC of {:.4f}\".format(accuracy_lr, roc_auc_lr))\n",
    "print(\"5. Random Forest achieved an accuracy of {:.4f} and ROC-AUC of {:.4f}\".format(accuracy_rf, roc_auc_rf))\n",
    "print(\"6. Key features identified as important for predicting heart disease include:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']} (importance: {row['importance']:.4f})\")\n",
    "\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"1. The dataset is relatively small with only\", len(df), \"records, which may limit model generalization.\")\n",
    "print(\"2. Missing values were imputed using median/mode, which may not capture the true underlying distribution.\")\n",
    "print(\"3. The dataset contains some categorical variables encoded as numbers that were treated as continuous.\")\n",
    "print(\"4. No feature engineering was performed to create new potentially meaningful features.\")\n",
    "print(\"5. Only basic models were used; more sophisticated techniques could potentially improve performance.\")\n",
    "\n",
    "print(\"\\nPossible Improvements:\")\n",
    "print(\"1. Collect more data to improve model generalization and robustness.\")\n",
    "print(\"2. Perform more sophisticated imputation techniques for missing values (e.g., KNN imputation).\")\n",
    "print(\"3. Engineer new features based on domain knowledge (e.g., cholesterol ratios, age groups).\")\n",
    "print(\"4. Try more advanced models like Gradient Boosting, SVM, or Neural Networks.\")\n",
    "print(\"5. Perform hyperparameter tuning using techniques like Grid Search or Random Search.\")\n",
    "print(\"6. Apply cross-validation for more robust model evaluation.\")\n",
    "print(\"7. Address class imbalance if present using techniques like SMOTE or class weights.\")\n",
    "\n",
    "print(\"\\nThis project demonstrates a complete data science workflow including EDA, data preprocessing,\")\n",
    "print(\"modeling, and evaluation, following best practices for reproducible research.\")\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}